data_science:
  loss_func: CrossEntropy
  n_qubits: ${n_qubits}
  n_qubits_range_quant: ${n_qubits_range_quant}
  n_layers: ${n_layers}
  n_layers_range_quant: ${n_layers_range_quant}
  data_reupload: ${data_reupload}
  data_reupload_range_quant: ${data_reupload_range_quant}
  epochs: 20
  optimizer:
    # combined:
    #   name: Adam
    #   lr: 0.001
    split:
      classical:
        name: SGD
        lr: 0.03
      quantum:
        name: Adam
        lr: 0.04
  optimizer_choice:
    combined:
      name: [SGD, Adam, NGD]
      lr: [0.01, 0.1, 'log']
    split:
      classical:
        name: [SGD, Adam, NGD]
        lr: [0.01, 0.1, 'log']
      quantum:
        name: [SGD, Adam, NGD, QNG] #SPSA
        lr: [0.01, 0.1, 'log']
  TEST_SIZE: ${TEST_SIZE}
  TRAINING_SIZE: ${TRAINING_SIZE}
  classes: ${classes}
  quant_status: ${quant_status}
  
  torch_seed: ${torch_seed}

  # Optuna
  # number of trials within one study. Multiple processes with resume_study=True will add to this number
  n_trials: 40
  timeout: 10800 #30h
  # this allows to control which parameters are considered during optimization. Leave empty to enable all
  enabled_hyperparameters: [optimizer_choice]
  optuna_path: "studies/split_optimizer.db"
  optuna_sampler_seed:  # should be None, if n_jobs=1 and separate processes are triggered from console
  selective_optimization: False # if true, only optimize classical params
  resume_study: True
  n_jobs: 1
  run_id: "OptunaOptimization#001"

  pool_process: False # alternative pool processing
  pruner_startup_trials: 10
  pruner_warmup_steps: 5
  pruner_interval_steps: 1
  pruner_min_trials: 10

  # default the loss function value, but can also be any metric that is returned by the training method
  optimization_metric: Train_CrossEntropy